# üìâ Autoencoder Simples (MLP) para Reconstru√ß√£o de Imagens MNIST

Este projeto implementa um **Autoencoder** b√°sico usando uma **Multi-Layer Perceptron (MLP)**. O objetivo √© treinar a rede para comprimir e, em seguida, reconstruir as imagens do dataset **MNIST**. O *encoder* aprende uma representa√ß√£o de baixa dimens√£o que captura as caracter√≠sticas essenciais dos d√≠gitos.

## üß† Arquitetura do Modelo (MLP Autoencoder)

O Autoencoder (AE) possui uma arquitetura sim√©trica de tr√™s camadas:

| Camada | Fun√ß√£o | Dimens√µes | Par√¢metros Chave |
| :--- | :--- | :--- | :--- |
| **Input (E)** | Recebe a Imagem | 784 neur√¥nios | $28 \times 28$ pixels (MNIST) |
| **Hidden (H)** | **Encoded Representation** (Latent Space) | `--hidden` neur√¥nios | **Compress√£o de Caracter√≠sticas** |
| **Output (O)** | Reconstru√ß√£o da Imagem | 784 neur√¥nios | Tenta igualar a camada Input |

As principais etapas do treinamento s√£o:

1.  **Forward Propagation:**
    * A imagem de entrada √© mapeada para a camada oculta (`H`).
    * A camada oculta (`H`) √© mapeada para a camada de sa√≠da (`O`), que √© a imagem reconstru√≠da ($\hat{X}$).
    * A fun√ß√£o de ativa√ß√£o utilizada √© a **Sigmoid** para ambas as camadas.

2.  **Backpropagation:**
    * O erro √© calculado usando a fun√ß√£o de perda **Mean Squared Error (MSE)** entre a entrada original ($X$) e a sa√≠da reconstru√≠da ($\hat{X}$).
    * O erro √© propagado de volta para ajustar os pesos ($\mathbf{W_h}$ e $\mathbf{W_o}$) usando a **Taxa de Aprendizado** (`--lr`).

## ‚öôÔ∏è Como Rodar o Projeto

### 1. Pr√©-requisitos

Instale as bibliotecas Python necess√°rias:

```bash
pip install numpy matplotlib scikit-learn
```

### 2. Execu√ß√£o

O script utiliza o m√≥dulo argparse para receber par√¢metros de linha de comando. Use o seguinte formato para executar o treinamento:

```bash
python MLP_autoencoder.py --hidden 128 --epochs 20 --lr 0.05 --log-every 1
```
